/* automatically generated by rust-bindgen */

pub const __CUDA_API_VERSION : u32 = 9020 ; pub const CUDA_VERSION : u32 = 9020 ; pub type CUdeviceptr = :: std :: os :: raw :: c_ulonglong ; pub type CUdevice = :: std :: os :: raw :: c_int ; # [ repr ( C ) ] # [ derive ( Debug , Copy , Clone ) ] pub struct CUctx_st { _unused : [ u8 ; 0 ] , } pub type CUcontext = * mut CUctx_st ; # [ repr ( C ) ] # [ derive ( Debug , Copy , Clone ) ] pub struct CUmod_st { _unused : [ u8 ; 0 ] , } pub type CUmodule = * mut CUmod_st ; # [ repr ( C ) ] # [ derive ( Debug , Copy , Clone ) ] pub struct CUfunc_st { _unused : [ u8 ; 0 ] , } pub type CUfunction = * mut CUfunc_st ; # [ repr ( C ) ] # [ derive ( Debug , Copy , Clone ) ] pub struct CUevent_st { _unused : [ u8 ; 0 ] , } pub type CUevent = * mut CUevent_st ; # [ repr ( C ) ] # [ derive ( Debug , Copy , Clone ) ] pub struct CUstream_st { _unused : [ u8 ; 0 ] , } pub type CUstream = * mut CUstream_st ; # [ repr ( C ) ] pub struct CUuuid_st { pub bytes : [ :: std :: os :: raw :: c_char ; 16usize ] , } # [ test ] fn bindgen_test_layout_CUuuid_st ( ) { assert_eq ! ( :: std :: mem :: size_of :: < CUuuid_st > ( ) , 16usize , concat ! ( "Size of: " , stringify ! ( CUuuid_st ) ) ) ; assert_eq ! ( :: std :: mem :: align_of :: < CUuuid_st > ( ) , 1usize , concat ! ( "Alignment of " , stringify ! ( CUuuid_st ) ) ) ; assert_eq ! ( unsafe { & ( * ( :: std :: ptr :: null :: < CUuuid_st > ( ) ) ) . bytes as * const _ as usize } , 0usize , concat ! ( "Offset of field: " , stringify ! ( CUuuid_st ) , "::" , stringify ! ( bytes ) ) ) ; } pub type CUuuid = CUuuid_st ; # [ doc = "< Maximum number of threads per block" ] pub const CUdevice_attribute_enum_CU_DEVICE_ATTRIBUTE_MAX_THREADS_PER_BLOCK : CUdevice_attribute_enum = 1 ; # [ doc = "< Maximum block dimension X" ] pub const CUdevice_attribute_enum_CU_DEVICE_ATTRIBUTE_MAX_BLOCK_DIM_X : CUdevice_attribute_enum = 2 ; # [ doc = "< Maximum block dimension Y" ] pub const CUdevice_attribute_enum_CU_DEVICE_ATTRIBUTE_MAX_BLOCK_DIM_Y : CUdevice_attribute_enum = 3 ; # [ doc = "< Maximum block dimension Z" ] pub const CUdevice_attribute_enum_CU_DEVICE_ATTRIBUTE_MAX_BLOCK_DIM_Z : CUdevice_attribute_enum = 4 ; # [ doc = "< Maximum grid dimension X" ] pub const CUdevice_attribute_enum_CU_DEVICE_ATTRIBUTE_MAX_GRID_DIM_X : CUdevice_attribute_enum = 5 ; # [ doc = "< Maximum grid dimension Y" ] pub const CUdevice_attribute_enum_CU_DEVICE_ATTRIBUTE_MAX_GRID_DIM_Y : CUdevice_attribute_enum = 6 ; # [ doc = "< Maximum grid dimension Z" ] pub const CUdevice_attribute_enum_CU_DEVICE_ATTRIBUTE_MAX_GRID_DIM_Z : CUdevice_attribute_enum = 7 ; # [ doc = "< Maximum shared memory available per block in bytes" ] pub const CUdevice_attribute_enum_CU_DEVICE_ATTRIBUTE_MAX_SHARED_MEMORY_PER_BLOCK : CUdevice_attribute_enum = 8 ; # [ doc = "< Deprecated, use CU_DEVICE_ATTRIBUTE_MAX_SHARED_MEMORY_PER_BLOCK" ] pub const CUdevice_attribute_enum_CU_DEVICE_ATTRIBUTE_SHARED_MEMORY_PER_BLOCK : CUdevice_attribute_enum = 8 ; # [ doc = "< Memory available on device for __constant__ variables in a CUDA C kernel in bytes" ] pub const CUdevice_attribute_enum_CU_DEVICE_ATTRIBUTE_TOTAL_CONSTANT_MEMORY : CUdevice_attribute_enum = 9 ; # [ doc = "< Warp size in threads" ] pub const CUdevice_attribute_enum_CU_DEVICE_ATTRIBUTE_WARP_SIZE : CUdevice_attribute_enum = 10 ; # [ doc = "< Maximum pitch in bytes allowed by memory copies" ] pub const CUdevice_attribute_enum_CU_DEVICE_ATTRIBUTE_MAX_PITCH : CUdevice_attribute_enum = 11 ; # [ doc = "< Maximum number of 32-bit registers available per block" ] pub const CUdevice_attribute_enum_CU_DEVICE_ATTRIBUTE_MAX_REGISTERS_PER_BLOCK : CUdevice_attribute_enum = 12 ; # [ doc = "< Deprecated, use CU_DEVICE_ATTRIBUTE_MAX_REGISTERS_PER_BLOCK" ] pub const CUdevice_attribute_enum_CU_DEVICE_ATTRIBUTE_REGISTERS_PER_BLOCK : CUdevice_attribute_enum = 12 ; # [ doc = "< Typical clock frequency in kilohertz" ] pub const CUdevice_attribute_enum_CU_DEVICE_ATTRIBUTE_CLOCK_RATE : CUdevice_attribute_enum = 13 ; # [ doc = "< Alignment requirement for textures" ] pub const CUdevice_attribute_enum_CU_DEVICE_ATTRIBUTE_TEXTURE_ALIGNMENT : CUdevice_attribute_enum = 14 ; # [ doc = "< Device can possibly copy memory and execute a kernel concurrently. Deprecated. Use instead CU_DEVICE_ATTRIBUTE_ASYNC_ENGINE_COUNT." ] pub const CUdevice_attribute_enum_CU_DEVICE_ATTRIBUTE_GPU_OVERLAP : CUdevice_attribute_enum = 15 ; # [ doc = "< Number of multiprocessors on device" ] pub const CUdevice_attribute_enum_CU_DEVICE_ATTRIBUTE_MULTIPROCESSOR_COUNT : CUdevice_attribute_enum = 16 ; # [ doc = "< Specifies whether there is a run time limit on kernels" ] pub const CUdevice_attribute_enum_CU_DEVICE_ATTRIBUTE_KERNEL_EXEC_TIMEOUT : CUdevice_attribute_enum = 17 ; # [ doc = "< Device is integrated with host memory" ] pub const CUdevice_attribute_enum_CU_DEVICE_ATTRIBUTE_INTEGRATED : CUdevice_attribute_enum = 18 ; # [ doc = "< Device can map host memory into CUDA address space" ] pub const CUdevice_attribute_enum_CU_DEVICE_ATTRIBUTE_CAN_MAP_HOST_MEMORY : CUdevice_attribute_enum = 19 ; # [ doc = "< Compute mode (See ::CUcomputemode for details)" ] pub const CUdevice_attribute_enum_CU_DEVICE_ATTRIBUTE_COMPUTE_MODE : CUdevice_attribute_enum = 20 ; # [ doc = "< Maximum 1D texture width" ] pub const CUdevice_attribute_enum_CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE1D_WIDTH : CUdevice_attribute_enum = 21 ; # [ doc = "< Maximum 2D texture width" ] pub const CUdevice_attribute_enum_CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE2D_WIDTH : CUdevice_attribute_enum = 22 ; # [ doc = "< Maximum 2D texture height" ] pub const CUdevice_attribute_enum_CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE2D_HEIGHT : CUdevice_attribute_enum = 23 ; # [ doc = "< Maximum 3D texture width" ] pub const CUdevice_attribute_enum_CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE3D_WIDTH : CUdevice_attribute_enum = 24 ; # [ doc = "< Maximum 3D texture height" ] pub const CUdevice_attribute_enum_CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE3D_HEIGHT : CUdevice_attribute_enum = 25 ; # [ doc = "< Maximum 3D texture depth" ] pub const CUdevice_attribute_enum_CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE3D_DEPTH : CUdevice_attribute_enum = 26 ; # [ doc = "< Maximum 2D layered texture width" ] pub const CUdevice_attribute_enum_CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE2D_LAYERED_WIDTH : CUdevice_attribute_enum = 27 ; # [ doc = "< Maximum 2D layered texture height" ] pub const CUdevice_attribute_enum_CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE2D_LAYERED_HEIGHT : CUdevice_attribute_enum = 28 ; # [ doc = "< Maximum layers in a 2D layered texture" ] pub const CUdevice_attribute_enum_CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE2D_LAYERED_LAYERS : CUdevice_attribute_enum = 29 ; # [ doc = "< Deprecated, use CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE2D_LAYERED_WIDTH" ] pub const CUdevice_attribute_enum_CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE2D_ARRAY_WIDTH : CUdevice_attribute_enum = 27 ; # [ doc = "< Deprecated, use CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE2D_LAYERED_HEIGHT" ] pub const CUdevice_attribute_enum_CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE2D_ARRAY_HEIGHT : CUdevice_attribute_enum = 28 ; # [ doc = "< Deprecated, use CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE2D_LAYERED_LAYERS" ] pub const CUdevice_attribute_enum_CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE2D_ARRAY_NUMSLICES : CUdevice_attribute_enum = 29 ; # [ doc = "< Alignment requirement for surfaces" ] pub const CUdevice_attribute_enum_CU_DEVICE_ATTRIBUTE_SURFACE_ALIGNMENT : CUdevice_attribute_enum = 30 ; # [ doc = "< Device can possibly execute multiple kernels concurrently" ] pub const CUdevice_attribute_enum_CU_DEVICE_ATTRIBUTE_CONCURRENT_KERNELS : CUdevice_attribute_enum = 31 ; # [ doc = "< Device has ECC support enabled" ] pub const CUdevice_attribute_enum_CU_DEVICE_ATTRIBUTE_ECC_ENABLED : CUdevice_attribute_enum = 32 ; # [ doc = "< PCI bus ID of the device" ] pub const CUdevice_attribute_enum_CU_DEVICE_ATTRIBUTE_PCI_BUS_ID : CUdevice_attribute_enum = 33 ; # [ doc = "< PCI device ID of the device" ] pub const CUdevice_attribute_enum_CU_DEVICE_ATTRIBUTE_PCI_DEVICE_ID : CUdevice_attribute_enum = 34 ; # [ doc = "< Device is using TCC driver model" ] pub const CUdevice_attribute_enum_CU_DEVICE_ATTRIBUTE_TCC_DRIVER : CUdevice_attribute_enum = 35 ; # [ doc = "< Peak memory clock frequency in kilohertz" ] pub const CUdevice_attribute_enum_CU_DEVICE_ATTRIBUTE_MEMORY_CLOCK_RATE : CUdevice_attribute_enum = 36 ; # [ doc = "< Global memory bus width in bits" ] pub const CUdevice_attribute_enum_CU_DEVICE_ATTRIBUTE_GLOBAL_MEMORY_BUS_WIDTH : CUdevice_attribute_enum = 37 ; # [ doc = "< Size of L2 cache in bytes" ] pub const CUdevice_attribute_enum_CU_DEVICE_ATTRIBUTE_L2_CACHE_SIZE : CUdevice_attribute_enum = 38 ; # [ doc = "< Maximum resident threads per multiprocessor" ] pub const CUdevice_attribute_enum_CU_DEVICE_ATTRIBUTE_MAX_THREADS_PER_MULTIPROCESSOR : CUdevice_attribute_enum = 39 ; # [ doc = "< Number of asynchronous engines" ] pub const CUdevice_attribute_enum_CU_DEVICE_ATTRIBUTE_ASYNC_ENGINE_COUNT : CUdevice_attribute_enum = 40 ; # [ doc = "< Device shares a unified address space with the host" ] pub const CUdevice_attribute_enum_CU_DEVICE_ATTRIBUTE_UNIFIED_ADDRESSING : CUdevice_attribute_enum = 41 ; # [ doc = "< Maximum 1D layered texture width" ] pub const CUdevice_attribute_enum_CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE1D_LAYERED_WIDTH : CUdevice_attribute_enum = 42 ; # [ doc = "< Maximum layers in a 1D layered texture" ] pub const CUdevice_attribute_enum_CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE1D_LAYERED_LAYERS : CUdevice_attribute_enum = 43 ; # [ doc = "< Deprecated, do not use." ] pub const CUdevice_attribute_enum_CU_DEVICE_ATTRIBUTE_CAN_TEX2D_GATHER : CUdevice_attribute_enum = 44 ; # [ doc = "< Maximum 2D texture width if CUDA_ARRAY3D_TEXTURE_GATHER is set" ] pub const CUdevice_attribute_enum_CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE2D_GATHER_WIDTH : CUdevice_attribute_enum = 45 ; # [ doc = "< Maximum 2D texture height if CUDA_ARRAY3D_TEXTURE_GATHER is set" ] pub const CUdevice_attribute_enum_CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE2D_GATHER_HEIGHT : CUdevice_attribute_enum = 46 ; # [ doc = "< Alternate maximum 3D texture width" ] pub const CUdevice_attribute_enum_CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE3D_WIDTH_ALTERNATE : CUdevice_attribute_enum = 47 ; # [ doc = "< Alternate maximum 3D texture height" ] pub const CUdevice_attribute_enum_CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE3D_HEIGHT_ALTERNATE : CUdevice_attribute_enum = 48 ; # [ doc = "< Alternate maximum 3D texture depth" ] pub const CUdevice_attribute_enum_CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE3D_DEPTH_ALTERNATE : CUdevice_attribute_enum = 49 ; # [ doc = "< PCI domain ID of the device" ] pub const CUdevice_attribute_enum_CU_DEVICE_ATTRIBUTE_PCI_DOMAIN_ID : CUdevice_attribute_enum = 50 ; # [ doc = "< Pitch alignment requirement for textures" ] pub const CUdevice_attribute_enum_CU_DEVICE_ATTRIBUTE_TEXTURE_PITCH_ALIGNMENT : CUdevice_attribute_enum = 51 ; # [ doc = "< Maximum cubemap texture width/height" ] pub const CUdevice_attribute_enum_CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURECUBEMAP_WIDTH : CUdevice_attribute_enum = 52 ; # [ doc = "< Maximum cubemap layered texture width/height" ] pub const CUdevice_attribute_enum_CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURECUBEMAP_LAYERED_WIDTH : CUdevice_attribute_enum = 53 ; # [ doc = "< Maximum layers in a cubemap layered texture" ] pub const CUdevice_attribute_enum_CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURECUBEMAP_LAYERED_LAYERS : CUdevice_attribute_enum = 54 ; # [ doc = "< Maximum 1D surface width" ] pub const CUdevice_attribute_enum_CU_DEVICE_ATTRIBUTE_MAXIMUM_SURFACE1D_WIDTH : CUdevice_attribute_enum = 55 ; # [ doc = "< Maximum 2D surface width" ] pub const CUdevice_attribute_enum_CU_DEVICE_ATTRIBUTE_MAXIMUM_SURFACE2D_WIDTH : CUdevice_attribute_enum = 56 ; # [ doc = "< Maximum 2D surface height" ] pub const CUdevice_attribute_enum_CU_DEVICE_ATTRIBUTE_MAXIMUM_SURFACE2D_HEIGHT : CUdevice_attribute_enum = 57 ; # [ doc = "< Maximum 3D surface width" ] pub const CUdevice_attribute_enum_CU_DEVICE_ATTRIBUTE_MAXIMUM_SURFACE3D_WIDTH : CUdevice_attribute_enum = 58 ; # [ doc = "< Maximum 3D surface height" ] pub const CUdevice_attribute_enum_CU_DEVICE_ATTRIBUTE_MAXIMUM_SURFACE3D_HEIGHT : CUdevice_attribute_enum = 59 ; # [ doc = "< Maximum 3D surface depth" ] pub const CUdevice_attribute_enum_CU_DEVICE_ATTRIBUTE_MAXIMUM_SURFACE3D_DEPTH : CUdevice_attribute_enum = 60 ; # [ doc = "< Maximum 1D layered surface width" ] pub const CUdevice_attribute_enum_CU_DEVICE_ATTRIBUTE_MAXIMUM_SURFACE1D_LAYERED_WIDTH : CUdevice_attribute_enum = 61 ; # [ doc = "< Maximum layers in a 1D layered surface" ] pub const CUdevice_attribute_enum_CU_DEVICE_ATTRIBUTE_MAXIMUM_SURFACE1D_LAYERED_LAYERS : CUdevice_attribute_enum = 62 ; # [ doc = "< Maximum 2D layered surface width" ] pub const CUdevice_attribute_enum_CU_DEVICE_ATTRIBUTE_MAXIMUM_SURFACE2D_LAYERED_WIDTH : CUdevice_attribute_enum = 63 ; # [ doc = "< Maximum 2D layered surface height" ] pub const CUdevice_attribute_enum_CU_DEVICE_ATTRIBUTE_MAXIMUM_SURFACE2D_LAYERED_HEIGHT : CUdevice_attribute_enum = 64 ; # [ doc = "< Maximum layers in a 2D layered surface" ] pub const CUdevice_attribute_enum_CU_DEVICE_ATTRIBUTE_MAXIMUM_SURFACE2D_LAYERED_LAYERS : CUdevice_attribute_enum = 65 ; # [ doc = "< Maximum cubemap surface width" ] pub const CUdevice_attribute_enum_CU_DEVICE_ATTRIBUTE_MAXIMUM_SURFACECUBEMAP_WIDTH : CUdevice_attribute_enum = 66 ; # [ doc = "< Maximum cubemap layered surface width" ] pub const CUdevice_attribute_enum_CU_DEVICE_ATTRIBUTE_MAXIMUM_SURFACECUBEMAP_LAYERED_WIDTH : CUdevice_attribute_enum = 67 ; # [ doc = "< Maximum layers in a cubemap layered surface" ] pub const CUdevice_attribute_enum_CU_DEVICE_ATTRIBUTE_MAXIMUM_SURFACECUBEMAP_LAYERED_LAYERS : CUdevice_attribute_enum = 68 ; # [ doc = "< Maximum 1D linear texture width" ] pub const CUdevice_attribute_enum_CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE1D_LINEAR_WIDTH : CUdevice_attribute_enum = 69 ; # [ doc = "< Maximum 2D linear texture width" ] pub const CUdevice_attribute_enum_CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE2D_LINEAR_WIDTH : CUdevice_attribute_enum = 70 ; # [ doc = "< Maximum 2D linear texture height" ] pub const CUdevice_attribute_enum_CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE2D_LINEAR_HEIGHT : CUdevice_attribute_enum = 71 ; # [ doc = "< Maximum 2D linear texture pitch in bytes" ] pub const CUdevice_attribute_enum_CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE2D_LINEAR_PITCH : CUdevice_attribute_enum = 72 ; # [ doc = "< Maximum mipmapped 2D texture width" ] pub const CUdevice_attribute_enum_CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE2D_MIPMAPPED_WIDTH : CUdevice_attribute_enum = 73 ; # [ doc = "< Maximum mipmapped 2D texture height" ] pub const CUdevice_attribute_enum_CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE2D_MIPMAPPED_HEIGHT : CUdevice_attribute_enum = 74 ; # [ doc = "< Major compute capability version number" ] pub const CUdevice_attribute_enum_CU_DEVICE_ATTRIBUTE_COMPUTE_CAPABILITY_MAJOR : CUdevice_attribute_enum = 75 ; # [ doc = "< Minor compute capability version number" ] pub const CUdevice_attribute_enum_CU_DEVICE_ATTRIBUTE_COMPUTE_CAPABILITY_MINOR : CUdevice_attribute_enum = 76 ; # [ doc = "< Maximum mipmapped 1D texture width" ] pub const CUdevice_attribute_enum_CU_DEVICE_ATTRIBUTE_MAXIMUM_TEXTURE1D_MIPMAPPED_WIDTH : CUdevice_attribute_enum = 77 ; # [ doc = "< Device supports stream priorities" ] pub const CUdevice_attribute_enum_CU_DEVICE_ATTRIBUTE_STREAM_PRIORITIES_SUPPORTED : CUdevice_attribute_enum = 78 ; # [ doc = "< Device supports caching globals in L1" ] pub const CUdevice_attribute_enum_CU_DEVICE_ATTRIBUTE_GLOBAL_L1_CACHE_SUPPORTED : CUdevice_attribute_enum = 79 ; # [ doc = "< Device supports caching locals in L1" ] pub const CUdevice_attribute_enum_CU_DEVICE_ATTRIBUTE_LOCAL_L1_CACHE_SUPPORTED : CUdevice_attribute_enum = 80 ; # [ doc = "< Maximum shared memory available per multiprocessor in bytes" ] pub const CUdevice_attribute_enum_CU_DEVICE_ATTRIBUTE_MAX_SHARED_MEMORY_PER_MULTIPROCESSOR : CUdevice_attribute_enum = 81 ; # [ doc = "< Maximum number of 32-bit registers available per multiprocessor" ] pub const CUdevice_attribute_enum_CU_DEVICE_ATTRIBUTE_MAX_REGISTERS_PER_MULTIPROCESSOR : CUdevice_attribute_enum = 82 ; # [ doc = "< Device can allocate managed memory on this system" ] pub const CUdevice_attribute_enum_CU_DEVICE_ATTRIBUTE_MANAGED_MEMORY : CUdevice_attribute_enum = 83 ; # [ doc = "< Device is on a multi-GPU board" ] pub const CUdevice_attribute_enum_CU_DEVICE_ATTRIBUTE_MULTI_GPU_BOARD : CUdevice_attribute_enum = 84 ; # [ doc = "< Unique id for a group of devices on the same multi-GPU board" ] pub const CUdevice_attribute_enum_CU_DEVICE_ATTRIBUTE_MULTI_GPU_BOARD_GROUP_ID : CUdevice_attribute_enum = 85 ; # [ doc = "< Link between the device and the host supports native atomic operations (this is a placeholder attribute, and is not supported on any current hardware)" ] pub const CUdevice_attribute_enum_CU_DEVICE_ATTRIBUTE_HOST_NATIVE_ATOMIC_SUPPORTED : CUdevice_attribute_enum = 86 ; # [ doc = "< Ratio of single precision performance (in floating-point operations per second) to double precision performance" ] pub const CUdevice_attribute_enum_CU_DEVICE_ATTRIBUTE_SINGLE_TO_DOUBLE_PRECISION_PERF_RATIO : CUdevice_attribute_enum = 87 ; # [ doc = "< Device supports coherently accessing pageable memory without calling cudaHostRegister on it" ] pub const CUdevice_attribute_enum_CU_DEVICE_ATTRIBUTE_PAGEABLE_MEMORY_ACCESS : CUdevice_attribute_enum = 88 ; # [ doc = "< Device can coherently access managed memory concurrently with the CPU" ] pub const CUdevice_attribute_enum_CU_DEVICE_ATTRIBUTE_CONCURRENT_MANAGED_ACCESS : CUdevice_attribute_enum = 89 ; # [ doc = "< Device supports compute preemption." ] pub const CUdevice_attribute_enum_CU_DEVICE_ATTRIBUTE_COMPUTE_PREEMPTION_SUPPORTED : CUdevice_attribute_enum = 90 ; # [ doc = "< Device can access host registered memory at the same virtual address as the CPU" ] pub const CUdevice_attribute_enum_CU_DEVICE_ATTRIBUTE_CAN_USE_HOST_POINTER_FOR_REGISTERED_MEM : CUdevice_attribute_enum = 91 ; # [ doc = "< ::cuStreamBatchMemOp and related APIs are supported." ] pub const CUdevice_attribute_enum_CU_DEVICE_ATTRIBUTE_CAN_USE_STREAM_MEM_OPS : CUdevice_attribute_enum = 92 ; # [ doc = "< 64-bit operations are supported in ::cuStreamBatchMemOp and related APIs." ] pub const CUdevice_attribute_enum_CU_DEVICE_ATTRIBUTE_CAN_USE_64_BIT_STREAM_MEM_OPS : CUdevice_attribute_enum = 93 ; # [ doc = "< ::CU_STREAM_WAIT_VALUE_NOR is supported." ] pub const CUdevice_attribute_enum_CU_DEVICE_ATTRIBUTE_CAN_USE_STREAM_WAIT_VALUE_NOR : CUdevice_attribute_enum = 94 ; # [ doc = "< Device supports launching cooperative kernels via ::cuLaunchCooperativeKernel" ] pub const CUdevice_attribute_enum_CU_DEVICE_ATTRIBUTE_COOPERATIVE_LAUNCH : CUdevice_attribute_enum = 95 ; # [ doc = "< Device can participate in cooperative kernels launched via ::cuLaunchCooperativeKernelMultiDevice" ] pub const CUdevice_attribute_enum_CU_DEVICE_ATTRIBUTE_COOPERATIVE_MULTI_DEVICE_LAUNCH : CUdevice_attribute_enum = 96 ; # [ doc = "< Maximum optin shared memory per block" ] pub const CUdevice_attribute_enum_CU_DEVICE_ATTRIBUTE_MAX_SHARED_MEMORY_PER_BLOCK_OPTIN : CUdevice_attribute_enum = 97 ; # [ doc = "< Both the ::CU_STREAM_WAIT_VALUE_FLUSH flag and the ::CU_STREAM_MEM_OP_FLUSH_REMOTE_WRITES MemOp are supported on the device. See \\ref CUDA_MEMOP for additional details." ] pub const CUdevice_attribute_enum_CU_DEVICE_ATTRIBUTE_CAN_FLUSH_REMOTE_WRITES : CUdevice_attribute_enum = 98 ; # [ doc = "< Device supports host memory registration via ::cudaHostRegister." ] pub const CUdevice_attribute_enum_CU_DEVICE_ATTRIBUTE_HOST_REGISTER_SUPPORTED : CUdevice_attribute_enum = 99 ; # [ doc = "< Device accesses pageable memory via the host\'s page tables." ] pub const CUdevice_attribute_enum_CU_DEVICE_ATTRIBUTE_PAGEABLE_MEMORY_ACCESS_USES_HOST_PAGE_TABLES : CUdevice_attribute_enum = 100 ; # [ doc = "< The host can directly access managed memory on the device without migration." ] pub const CUdevice_attribute_enum_CU_DEVICE_ATTRIBUTE_DIRECT_MANAGED_MEM_ACCESS_FROM_HOST : CUdevice_attribute_enum = 101 ; pub const CUdevice_attribute_enum_CU_DEVICE_ATTRIBUTE_MAX : CUdevice_attribute_enum = 102 ; # [ doc = " Device properties" ] pub type CUdevice_attribute_enum = u32 ; pub use self :: CUdevice_attribute_enum as CUdevice_attribute ; # [ doc = " Max number of registers that a thread may use.\\n" ] # [ doc = " Option type: unsigned int\\n" ] # [ doc = " Applies to: compiler only" ] pub const CUjit_option_enum_CU_JIT_MAX_REGISTERS : CUjit_option_enum = 0 ; # [ doc = " IN: Specifies minimum number of threads per block to target compilation" ] # [ doc = " for\\n" ] # [ doc = " OUT: Returns the number of threads the compiler actually targeted." ] # [ doc = " This restricts the resource utilization fo the compiler (e.g. max" ] # [ doc = " registers) such that a block with the given number of threads should be" ] # [ doc = " able to launch based on register limitations. Note, this option does not" ] # [ doc = " currently take into account any other resource limitations, such as" ] # [ doc = " shared memory utilization.\\n" ] # [ doc = " Cannot be combined with ::CU_JIT_TARGET.\\n" ] # [ doc = " Option type: unsigned int\\n" ] # [ doc = " Applies to: compiler only" ] pub const CUjit_option_enum_CU_JIT_THREADS_PER_BLOCK : CUjit_option_enum = 1 ; # [ doc = " Overwrites the option value with the total wall clock time, in" ] # [ doc = " milliseconds, spent in the compiler and linker\\n" ] # [ doc = " Option type: float\\n" ] # [ doc = " Applies to: compiler and linker" ] pub const CUjit_option_enum_CU_JIT_WALL_TIME : CUjit_option_enum = 2 ; # [ doc = " Pointer to a buffer in which to print any log messages" ] # [ doc = " that are informational in nature (the buffer size is specified via" ] # [ doc = " option ::CU_JIT_INFO_LOG_BUFFER_SIZE_BYTES)\\n" ] # [ doc = " Option type: char *\\n" ] # [ doc = " Applies to: compiler and linker" ] pub const CUjit_option_enum_CU_JIT_INFO_LOG_BUFFER : CUjit_option_enum = 3 ; # [ doc = " IN: Log buffer size in bytes.  Log messages will be capped at this size" ] # [ doc = " (including null terminator)\\n" ] # [ doc = " OUT: Amount of log buffer filled with messages\\n" ] # [ doc = " Option type: unsigned int\\n" ] # [ doc = " Applies to: compiler and linker" ] pub const CUjit_option_enum_CU_JIT_INFO_LOG_BUFFER_SIZE_BYTES : CUjit_option_enum = 4 ; # [ doc = " Pointer to a buffer in which to print any log messages that" ] # [ doc = " reflect errors (the buffer size is specified via option" ] # [ doc = " ::CU_JIT_ERROR_LOG_BUFFER_SIZE_BYTES)\\n" ] # [ doc = " Option type: char *\\n" ] # [ doc = " Applies to: compiler and linker" ] pub const CUjit_option_enum_CU_JIT_ERROR_LOG_BUFFER : CUjit_option_enum = 5 ; # [ doc = " IN: Log buffer size in bytes.  Log messages will be capped at this size" ] # [ doc = " (including null terminator)\\n" ] # [ doc = " OUT: Amount of log buffer filled with messages\\n" ] # [ doc = " Option type: unsigned int\\n" ] # [ doc = " Applies to: compiler and linker" ] pub const CUjit_option_enum_CU_JIT_ERROR_LOG_BUFFER_SIZE_BYTES : CUjit_option_enum = 6 ; # [ doc = " Level of optimizations to apply to generated code (0 - 4), with 4" ] # [ doc = " being the default and highest level of optimizations.\\n" ] # [ doc = " Option type: unsigned int\\n" ] # [ doc = " Applies to: compiler only" ] pub const CUjit_option_enum_CU_JIT_OPTIMIZATION_LEVEL : CUjit_option_enum = 7 ; # [ doc = " No option value required. Determines the target based on the current" ] # [ doc = " attached context (default)\\n" ] # [ doc = " Option type: No option value needed\\n" ] # [ doc = " Applies to: compiler and linker" ] pub const CUjit_option_enum_CU_JIT_TARGET_FROM_CUCONTEXT : CUjit_option_enum = 8 ; # [ doc = " Target is chosen based on supplied ::CUjit_target.  Cannot be" ] # [ doc = " combined with ::CU_JIT_THREADS_PER_BLOCK.\\n" ] # [ doc = " Option type: unsigned int for enumerated type ::CUjit_target\\n" ] # [ doc = " Applies to: compiler and linker" ] pub const CUjit_option_enum_CU_JIT_TARGET : CUjit_option_enum = 9 ; # [ doc = " Specifies choice of fallback strategy if matching cubin is not found." ] # [ doc = " Choice is based on supplied ::CUjit_fallback.  This option cannot be" ] # [ doc = " used with cuLink* APIs as the linker requires exact matches.\\n" ] # [ doc = " Option type: unsigned int for enumerated type ::CUjit_fallback\\n" ] # [ doc = " Applies to: compiler only" ] pub const CUjit_option_enum_CU_JIT_FALLBACK_STRATEGY : CUjit_option_enum = 10 ; # [ doc = " Specifies whether to create debug information in output (-g)" ] # [ doc = " (0: false, default)\\n" ] # [ doc = " Option type: int\\n" ] # [ doc = " Applies to: compiler and linker" ] pub const CUjit_option_enum_CU_JIT_GENERATE_DEBUG_INFO : CUjit_option_enum = 11 ; # [ doc = " Generate verbose log messages (0: false, default)\\n" ] # [ doc = " Option type: int\\n" ] # [ doc = " Applies to: compiler and linker" ] pub const CUjit_option_enum_CU_JIT_LOG_VERBOSE : CUjit_option_enum = 12 ; # [ doc = " Generate line number information (-lineinfo) (0: false, default)\\n" ] # [ doc = " Option type: int\\n" ] # [ doc = " Applies to: compiler only" ] pub const CUjit_option_enum_CU_JIT_GENERATE_LINE_INFO : CUjit_option_enum = 13 ; # [ doc = " Specifies whether to enable caching explicitly (-dlcm) \\n" ] # [ doc = " Choice is based on supplied ::CUjit_cacheMode_enum.\\n" ] # [ doc = " Option type: unsigned int for enumerated type ::CUjit_cacheMode_enum\\n" ] # [ doc = " Applies to: compiler only" ] pub const CUjit_option_enum_CU_JIT_CACHE_MODE : CUjit_option_enum = 14 ; # [ doc = " The below jit options are used for internal purposes only, in this version of CUDA" ] pub const CUjit_option_enum_CU_JIT_NEW_SM3X_OPT : CUjit_option_enum = 15 ; # [ doc = " The below jit options are used for internal purposes only, in this version of CUDA" ] pub const CUjit_option_enum_CU_JIT_FAST_COMPILE : CUjit_option_enum = 16 ; # [ doc = " The below jit options are used for internal purposes only, in this version of CUDA" ] pub const CUjit_option_enum_CU_JIT_NUM_OPTIONS : CUjit_option_enum = 17 ; # [ doc = " Online compiler and linker options" ] pub type CUjit_option_enum = u32 ; pub use self :: CUjit_option_enum as CUjit_option ; # [ doc = " The API call returned with no errors. In the case of query calls, this" ] # [ doc = " can also mean that the operation being queried is complete (see" ] # [ doc = " ::cuEventQuery() and ::cuStreamQuery())." ] pub const cudaError_enum_CUDA_SUCCESS : cudaError_enum = 0 ; # [ doc = " This indicates that one or more of the parameters passed to the API call" ] # [ doc = " is not within an acceptable range of values." ] pub const cudaError_enum_CUDA_ERROR_INVALID_VALUE : cudaError_enum = 1 ; # [ doc = " The API call failed because it was unable to allocate enough memory to" ] # [ doc = " perform the requested operation." ] pub const cudaError_enum_CUDA_ERROR_OUT_OF_MEMORY : cudaError_enum = 2 ; # [ doc = " This indicates that the CUDA driver has not been initialized with" ] # [ doc = " ::cuInit() or that initialization has failed." ] pub const cudaError_enum_CUDA_ERROR_NOT_INITIALIZED : cudaError_enum = 3 ; # [ doc = " This indicates that the CUDA driver is in the process of shutting down." ] pub const cudaError_enum_CUDA_ERROR_DEINITIALIZED : cudaError_enum = 4 ; # [ doc = " This indicates profiler is not initialized for this run. This can" ] # [ doc = " happen when the application is running with external profiling tools" ] # [ doc = " like visual profiler." ] pub const cudaError_enum_CUDA_ERROR_PROFILER_DISABLED : cudaError_enum = 5 ; # [ doc = " \\deprecated" ] # [ doc = " This error return is deprecated as of CUDA 5.0. It is no longer an error" ] # [ doc = " to attempt to enable/disable the profiling via ::cuProfilerStart or" ] # [ doc = " ::cuProfilerStop without initialization." ] pub const cudaError_enum_CUDA_ERROR_PROFILER_NOT_INITIALIZED : cudaError_enum = 6 ; # [ doc = " \\deprecated" ] # [ doc = " This error return is deprecated as of CUDA 5.0. It is no longer an error" ] # [ doc = " to call cuProfilerStart() when profiling is already enabled." ] pub const cudaError_enum_CUDA_ERROR_PROFILER_ALREADY_STARTED : cudaError_enum = 7 ; # [ doc = " \\deprecated" ] # [ doc = " This error return is deprecated as of CUDA 5.0. It is no longer an error" ] # [ doc = " to call cuProfilerStop() when profiling is already disabled." ] pub const cudaError_enum_CUDA_ERROR_PROFILER_ALREADY_STOPPED : cudaError_enum = 8 ; # [ doc = " This indicates that no CUDA-capable devices were detected by the installed" ] # [ doc = " CUDA driver." ] pub const cudaError_enum_CUDA_ERROR_NO_DEVICE : cudaError_enum = 100 ; # [ doc = " This indicates that the device ordinal supplied by the user does not" ] # [ doc = " correspond to a valid CUDA device." ] pub const cudaError_enum_CUDA_ERROR_INVALID_DEVICE : cudaError_enum = 101 ; # [ doc = " This indicates that the device kernel image is invalid. This can also" ] # [ doc = " indicate an invalid CUDA module." ] pub const cudaError_enum_CUDA_ERROR_INVALID_IMAGE : cudaError_enum = 200 ; # [ doc = " This most frequently indicates that there is no context bound to the" ] # [ doc = " current thread. This can also be returned if the context passed to an" ] # [ doc = " API call is not a valid handle (such as a context that has had" ] # [ doc = " ::cuCtxDestroy() invoked on it). This can also be returned if a user" ] # [ doc = " mixes different API versions (i.e. 3010 context with 3020 API calls)." ] # [ doc = " See ::cuCtxGetApiVersion() for more details." ] pub const cudaError_enum_CUDA_ERROR_INVALID_CONTEXT : cudaError_enum = 201 ; # [ doc = " This indicated that the context being supplied as a parameter to the" ] # [ doc = " API call was already the active context." ] # [ doc = " \\deprecated" ] # [ doc = " This error return is deprecated as of CUDA 3.2. It is no longer an" ] # [ doc = " error to attempt to push the active context via ::cuCtxPushCurrent()." ] pub const cudaError_enum_CUDA_ERROR_CONTEXT_ALREADY_CURRENT : cudaError_enum = 202 ; # [ doc = " This indicates that a map or register operation has failed." ] pub const cudaError_enum_CUDA_ERROR_MAP_FAILED : cudaError_enum = 205 ; # [ doc = " This indicates that an unmap or unregister operation has failed." ] pub const cudaError_enum_CUDA_ERROR_UNMAP_FAILED : cudaError_enum = 206 ; # [ doc = " This indicates that the specified array is currently mapped and thus" ] # [ doc = " cannot be destroyed." ] pub const cudaError_enum_CUDA_ERROR_ARRAY_IS_MAPPED : cudaError_enum = 207 ; # [ doc = " This indicates that the resource is already mapped." ] pub const cudaError_enum_CUDA_ERROR_ALREADY_MAPPED : cudaError_enum = 208 ; # [ doc = " This indicates that there is no kernel image available that is suitable" ] # [ doc = " for the device. This can occur when a user specifies code generation" ] # [ doc = " options for a particular CUDA source file that do not include the" ] # [ doc = " corresponding device configuration." ] pub const cudaError_enum_CUDA_ERROR_NO_BINARY_FOR_GPU : cudaError_enum = 209 ; # [ doc = " This indicates that a resource has already been acquired." ] pub const cudaError_enum_CUDA_ERROR_ALREADY_ACQUIRED : cudaError_enum = 210 ; # [ doc = " This indicates that a resource is not mapped." ] pub const cudaError_enum_CUDA_ERROR_NOT_MAPPED : cudaError_enum = 211 ; # [ doc = " This indicates that a mapped resource is not available for access as an" ] # [ doc = " array." ] pub const cudaError_enum_CUDA_ERROR_NOT_MAPPED_AS_ARRAY : cudaError_enum = 212 ; # [ doc = " This indicates that a mapped resource is not available for access as a" ] # [ doc = " pointer." ] pub const cudaError_enum_CUDA_ERROR_NOT_MAPPED_AS_POINTER : cudaError_enum = 213 ; # [ doc = " This indicates that an uncorrectable ECC error was detected during" ] # [ doc = " execution." ] pub const cudaError_enum_CUDA_ERROR_ECC_UNCORRECTABLE : cudaError_enum = 214 ; # [ doc = " This indicates that the ::CUlimit passed to the API call is not" ] # [ doc = " supported by the active device." ] pub const cudaError_enum_CUDA_ERROR_UNSUPPORTED_LIMIT : cudaError_enum = 215 ; # [ doc = " This indicates that the ::CUcontext passed to the API call can" ] # [ doc = " only be bound to a single CPU thread at a time but is already" ] # [ doc = " bound to a CPU thread." ] pub const cudaError_enum_CUDA_ERROR_CONTEXT_ALREADY_IN_USE : cudaError_enum = 216 ; # [ doc = " This indicates that peer access is not supported across the given" ] # [ doc = " devices." ] pub const cudaError_enum_CUDA_ERROR_PEER_ACCESS_UNSUPPORTED : cudaError_enum = 217 ; # [ doc = " This indicates that a PTX JIT compilation failed." ] pub const cudaError_enum_CUDA_ERROR_INVALID_PTX : cudaError_enum = 218 ; # [ doc = " This indicates an error with OpenGL or DirectX context." ] pub const cudaError_enum_CUDA_ERROR_INVALID_GRAPHICS_CONTEXT : cudaError_enum = 219 ; # [ doc = " This indicates that an uncorrectable NVLink error was detected during the" ] # [ doc = " execution." ] pub const cudaError_enum_CUDA_ERROR_NVLINK_UNCORRECTABLE : cudaError_enum = 220 ; # [ doc = " This indicates that the PTX JIT compiler library was not found." ] pub const cudaError_enum_CUDA_ERROR_JIT_COMPILER_NOT_FOUND : cudaError_enum = 221 ; # [ doc = " This indicates that the device kernel source is invalid." ] pub const cudaError_enum_CUDA_ERROR_INVALID_SOURCE : cudaError_enum = 300 ; # [ doc = " This indicates that the file specified was not found." ] pub const cudaError_enum_CUDA_ERROR_FILE_NOT_FOUND : cudaError_enum = 301 ; # [ doc = " This indicates that a link to a shared object failed to resolve." ] pub const cudaError_enum_CUDA_ERROR_SHARED_OBJECT_SYMBOL_NOT_FOUND : cudaError_enum = 302 ; # [ doc = " This indicates that initialization of a shared object failed." ] pub const cudaError_enum_CUDA_ERROR_SHARED_OBJECT_INIT_FAILED : cudaError_enum = 303 ; # [ doc = " This indicates that an OS call failed." ] pub const cudaError_enum_CUDA_ERROR_OPERATING_SYSTEM : cudaError_enum = 304 ; # [ doc = " This indicates that a resource handle passed to the API call was not" ] # [ doc = " valid. Resource handles are opaque types like ::CUstream and ::CUevent." ] pub const cudaError_enum_CUDA_ERROR_INVALID_HANDLE : cudaError_enum = 400 ; # [ doc = " This indicates that a named symbol was not found. Examples of symbols" ] # [ doc = " are global/constant variable names, texture names, and surface names." ] pub const cudaError_enum_CUDA_ERROR_NOT_FOUND : cudaError_enum = 500 ; # [ doc = " This indicates that asynchronous operations issued previously have not" ] # [ doc = " completed yet. This result is not actually an error, but must be indicated" ] # [ doc = " differently than ::CUDA_SUCCESS (which indicates completion). Calls that" ] # [ doc = " may return this value include ::cuEventQuery() and ::cuStreamQuery()." ] pub const cudaError_enum_CUDA_ERROR_NOT_READY : cudaError_enum = 600 ; # [ doc = " While executing a kernel, the device encountered a" ] # [ doc = " load or store instruction on an invalid memory address." ] # [ doc = " This leaves the process in an inconsistent state and any further CUDA work" ] # [ doc = " will return the same error. To continue using CUDA, the process must be terminated" ] # [ doc = " and relaunched." ] pub const cudaError_enum_CUDA_ERROR_ILLEGAL_ADDRESS : cudaError_enum = 700 ; # [ doc = " This indicates that a launch did not occur because it did not have" ] # [ doc = " appropriate resources. This error usually indicates that the user has" ] # [ doc = " attempted to pass too many arguments to the device kernel, or the" ] # [ doc = " kernel launch specifies too many threads for the kernel\'s register" ] # [ doc = " count. Passing arguments of the wrong size (i.e. a 64-bit pointer" ] # [ doc = " when a 32-bit int is expected) is equivalent to passing too many" ] # [ doc = " arguments and can also result in this error." ] pub const cudaError_enum_CUDA_ERROR_LAUNCH_OUT_OF_RESOURCES : cudaError_enum = 701 ; # [ doc = " This indicates that the device kernel took too long to execute. This can" ] # [ doc = " only occur if timeouts are enabled - see the device attribute" ] # [ doc = " ::CU_DEVICE_ATTRIBUTE_KERNEL_EXEC_TIMEOUT for more information." ] # [ doc = " This leaves the process in an inconsistent state and any further CUDA work" ] # [ doc = " will return the same error. To continue using CUDA, the process must be terminated" ] # [ doc = " and relaunched." ] pub const cudaError_enum_CUDA_ERROR_LAUNCH_TIMEOUT : cudaError_enum = 702 ; # [ doc = " This error indicates a kernel launch that uses an incompatible texturing" ] # [ doc = " mode." ] pub const cudaError_enum_CUDA_ERROR_LAUNCH_INCOMPATIBLE_TEXTURING : cudaError_enum = 703 ; # [ doc = " This error indicates that a call to ::cuCtxEnablePeerAccess() is" ] # [ doc = " trying to re-enable peer access to a context which has already" ] # [ doc = " had peer access to it enabled." ] pub const cudaError_enum_CUDA_ERROR_PEER_ACCESS_ALREADY_ENABLED : cudaError_enum = 704 ; # [ doc = " This error indicates that ::cuCtxDisablePeerAccess() is" ] # [ doc = " trying to disable peer access which has not been enabled yet" ] # [ doc = " via ::cuCtxEnablePeerAccess()." ] pub const cudaError_enum_CUDA_ERROR_PEER_ACCESS_NOT_ENABLED : cudaError_enum = 705 ; # [ doc = " This error indicates that the primary context for the specified device" ] # [ doc = " has already been initialized." ] pub const cudaError_enum_CUDA_ERROR_PRIMARY_CONTEXT_ACTIVE : cudaError_enum = 708 ; # [ doc = " This error indicates that the context current to the calling thread" ] # [ doc = " has been destroyed using ::cuCtxDestroy, or is a primary context which" ] # [ doc = " has not yet been initialized." ] pub const cudaError_enum_CUDA_ERROR_CONTEXT_IS_DESTROYED : cudaError_enum = 709 ; # [ doc = " A device-side assert triggered during kernel execution. The context" ] # [ doc = " cannot be used anymore, and must be destroyed. All existing device" ] # [ doc = " memory allocations from this context are invalid and must be" ] # [ doc = " reconstructed if the program is to continue using CUDA." ] pub const cudaError_enum_CUDA_ERROR_ASSERT : cudaError_enum = 710 ; # [ doc = " This error indicates that the hardware resources required to enable" ] # [ doc = " peer access have been exhausted for one or more of the devices" ] # [ doc = " passed to ::cuCtxEnablePeerAccess()." ] pub const cudaError_enum_CUDA_ERROR_TOO_MANY_PEERS : cudaError_enum = 711 ; # [ doc = " This error indicates that the memory range passed to ::cuMemHostRegister()" ] # [ doc = " has already been registered." ] pub const cudaError_enum_CUDA_ERROR_HOST_MEMORY_ALREADY_REGISTERED : cudaError_enum = 712 ; # [ doc = " This error indicates that the pointer passed to ::cuMemHostUnregister()" ] # [ doc = " does not correspond to any currently registered memory region." ] pub const cudaError_enum_CUDA_ERROR_HOST_MEMORY_NOT_REGISTERED : cudaError_enum = 713 ; # [ doc = " While executing a kernel, the device encountered a stack error." ] # [ doc = " This can be due to stack corruption or exceeding the stack size limit." ] # [ doc = " This leaves the process in an inconsistent state and any further CUDA work" ] # [ doc = " will return the same error. To continue using CUDA, the process must be terminated" ] # [ doc = " and relaunched." ] pub const cudaError_enum_CUDA_ERROR_HARDWARE_STACK_ERROR : cudaError_enum = 714 ; # [ doc = " While executing a kernel, the device encountered an illegal instruction." ] # [ doc = " This leaves the process in an inconsistent state and any further CUDA work" ] # [ doc = " will return the same error. To continue using CUDA, the process must be terminated" ] # [ doc = " and relaunched." ] pub const cudaError_enum_CUDA_ERROR_ILLEGAL_INSTRUCTION : cudaError_enum = 715 ; # [ doc = " While executing a kernel, the device encountered a load or store instruction" ] # [ doc = " on a memory address which is not aligned." ] # [ doc = " This leaves the process in an inconsistent state and any further CUDA work" ] # [ doc = " will return the same error. To continue using CUDA, the process must be terminated" ] # [ doc = " and relaunched." ] pub const cudaError_enum_CUDA_ERROR_MISALIGNED_ADDRESS : cudaError_enum = 716 ; # [ doc = " While executing a kernel, the device encountered an instruction" ] # [ doc = " which can only operate on memory locations in certain address spaces" ] # [ doc = " (global, shared, or local), but was supplied a memory address not" ] # [ doc = " belonging to an allowed address space." ] # [ doc = " This leaves the process in an inconsistent state and any further CUDA work" ] # [ doc = " will return the same error. To continue using CUDA, the process must be terminated" ] # [ doc = " and relaunched." ] pub const cudaError_enum_CUDA_ERROR_INVALID_ADDRESS_SPACE : cudaError_enum = 717 ; # [ doc = " While executing a kernel, the device program counter wrapped its address space." ] # [ doc = " This leaves the process in an inconsistent state and any further CUDA work" ] # [ doc = " will return the same error. To continue using CUDA, the process must be terminated" ] # [ doc = " and relaunched." ] pub const cudaError_enum_CUDA_ERROR_INVALID_PC : cudaError_enum = 718 ; # [ doc = " An exception occurred on the device while executing a kernel. Common" ] # [ doc = " causes include dereferencing an invalid device pointer and accessing" ] # [ doc = " out of bounds shared memory." ] # [ doc = " This leaves the process in an inconsistent state and any further CUDA work" ] # [ doc = " will return the same error. To continue using CUDA, the process must be terminated" ] # [ doc = " and relaunched." ] pub const cudaError_enum_CUDA_ERROR_LAUNCH_FAILED : cudaError_enum = 719 ; # [ doc = " This error indicates that the number of blocks launched per grid for a kernel that was" ] # [ doc = " launched via either ::cuLaunchCooperativeKernel or ::cuLaunchCooperativeKernelMultiDevice" ] # [ doc = " exceeds the maximum number of blocks as allowed by ::cuOccupancyMaxActiveBlocksPerMultiprocessor" ] # [ doc = " or ::cuOccupancyMaxActiveBlocksPerMultiprocessorWithFlags times the number of multiprocessors" ] # [ doc = " as specified by the device attribute ::CU_DEVICE_ATTRIBUTE_MULTIPROCESSOR_COUNT." ] pub const cudaError_enum_CUDA_ERROR_COOPERATIVE_LAUNCH_TOO_LARGE : cudaError_enum = 720 ; # [ doc = " This error indicates that the attempted operation is not permitted." ] pub const cudaError_enum_CUDA_ERROR_NOT_PERMITTED : cudaError_enum = 800 ; # [ doc = " This error indicates that the attempted operation is not supported" ] # [ doc = " on the current system or device." ] pub const cudaError_enum_CUDA_ERROR_NOT_SUPPORTED : cudaError_enum = 801 ; # [ doc = " This indicates that an unknown internal error has occurred." ] pub const cudaError_enum_CUDA_ERROR_UNKNOWN : cudaError_enum = 999 ; # [ doc = " Error codes" ] pub type cudaError_enum = u32 ; pub use self :: cudaError_enum as CUresult ; # [ doc = " CUDA stream callback" ] # [ doc = " \\param hStream The stream the callback was added to, as passed to ::cuStreamAddCallback.  May be NULL." ] # [ doc = " \\param status ::CUDA_SUCCESS or any persistent error on the stream." ] # [ doc = " \\param userData User parameter provided at registration." ] pub type CUstreamCallback = :: std :: option :: Option < unsafe extern "C" fn ( hStream : CUstream , status : CUresult , userData : * mut :: std :: os :: raw :: c_void ) > ; # [ doc = " Kernel launch parameters" ] # [ repr ( C ) ] pub struct CUDA_LAUNCH_PARAMS_st { # [ doc = "< Kernel to launch" ] pub function : CUfunction , # [ doc = "< Width of grid in blocks" ] pub gridDimX : :: std :: os :: raw :: c_uint , # [ doc = "< Height of grid in blocks" ] pub gridDimY : :: std :: os :: raw :: c_uint , # [ doc = "< Depth of grid in blocks" ] pub gridDimZ : :: std :: os :: raw :: c_uint , # [ doc = "< X dimension of each thread block" ] pub blockDimX : :: std :: os :: raw :: c_uint , # [ doc = "< Y dimension of each thread block" ] pub blockDimY : :: std :: os :: raw :: c_uint , # [ doc = "< Z dimension of each thread block" ] pub blockDimZ : :: std :: os :: raw :: c_uint , # [ doc = "< Dynamic shared-memory size per thread block in bytes" ] pub sharedMemBytes : :: std :: os :: raw :: c_uint , # [ doc = "< Stream identifier" ] pub hStream : CUstream , # [ doc = "< Array of pointers to kernel parameters" ] pub kernelParams : * mut * mut :: std :: os :: raw :: c_void , } # [ test ] fn bindgen_test_layout_CUDA_LAUNCH_PARAMS_st ( ) { assert_eq ! ( :: std :: mem :: size_of :: < CUDA_LAUNCH_PARAMS_st > ( ) , 56usize , concat ! ( "Size of: " , stringify ! ( CUDA_LAUNCH_PARAMS_st ) ) ) ; assert_eq ! ( :: std :: mem :: align_of :: < CUDA_LAUNCH_PARAMS_st > ( ) , 8usize , concat ! ( "Alignment of " , stringify ! ( CUDA_LAUNCH_PARAMS_st ) ) ) ; assert_eq ! ( unsafe { & ( * ( :: std :: ptr :: null :: < CUDA_LAUNCH_PARAMS_st > ( ) ) ) . function as * const _ as usize } , 0usize , concat ! ( "Offset of field: " , stringify ! ( CUDA_LAUNCH_PARAMS_st ) , "::" , stringify ! ( function ) ) ) ; assert_eq ! ( unsafe { & ( * ( :: std :: ptr :: null :: < CUDA_LAUNCH_PARAMS_st > ( ) ) ) . gridDimX as * const _ as usize } , 8usize , concat ! ( "Offset of field: " , stringify ! ( CUDA_LAUNCH_PARAMS_st ) , "::" , stringify ! ( gridDimX ) ) ) ; assert_eq ! ( unsafe { & ( * ( :: std :: ptr :: null :: < CUDA_LAUNCH_PARAMS_st > ( ) ) ) . gridDimY as * const _ as usize } , 12usize , concat ! ( "Offset of field: " , stringify ! ( CUDA_LAUNCH_PARAMS_st ) , "::" , stringify ! ( gridDimY ) ) ) ; assert_eq ! ( unsafe { & ( * ( :: std :: ptr :: null :: < CUDA_LAUNCH_PARAMS_st > ( ) ) ) . gridDimZ as * const _ as usize } , 16usize , concat ! ( "Offset of field: " , stringify ! ( CUDA_LAUNCH_PARAMS_st ) , "::" , stringify ! ( gridDimZ ) ) ) ; assert_eq ! ( unsafe { & ( * ( :: std :: ptr :: null :: < CUDA_LAUNCH_PARAMS_st > ( ) ) ) . blockDimX as * const _ as usize } , 20usize , concat ! ( "Offset of field: " , stringify ! ( CUDA_LAUNCH_PARAMS_st ) , "::" , stringify ! ( blockDimX ) ) ) ; assert_eq ! ( unsafe { & ( * ( :: std :: ptr :: null :: < CUDA_LAUNCH_PARAMS_st > ( ) ) ) . blockDimY as * const _ as usize } , 24usize , concat ! ( "Offset of field: " , stringify ! ( CUDA_LAUNCH_PARAMS_st ) , "::" , stringify ! ( blockDimY ) ) ) ; assert_eq ! ( unsafe { & ( * ( :: std :: ptr :: null :: < CUDA_LAUNCH_PARAMS_st > ( ) ) ) . blockDimZ as * const _ as usize } , 28usize , concat ! ( "Offset of field: " , stringify ! ( CUDA_LAUNCH_PARAMS_st ) , "::" , stringify ! ( blockDimZ ) ) ) ; assert_eq ! ( unsafe { & ( * ( :: std :: ptr :: null :: < CUDA_LAUNCH_PARAMS_st > ( ) ) ) . sharedMemBytes as * const _ as usize } , 32usize , concat ! ( "Offset of field: " , stringify ! ( CUDA_LAUNCH_PARAMS_st ) , "::" , stringify ! ( sharedMemBytes ) ) ) ; assert_eq ! ( unsafe { & ( * ( :: std :: ptr :: null :: < CUDA_LAUNCH_PARAMS_st > ( ) ) ) . hStream as * const _ as usize } , 40usize , concat ! ( "Offset of field: " , stringify ! ( CUDA_LAUNCH_PARAMS_st ) , "::" , stringify ! ( hStream ) ) ) ; assert_eq ! ( unsafe { & ( * ( :: std :: ptr :: null :: < CUDA_LAUNCH_PARAMS_st > ( ) ) ) . kernelParams as * const _ as usize } , 48usize , concat ! ( "Offset of field: " , stringify ! ( CUDA_LAUNCH_PARAMS_st ) , "::" , stringify ! ( kernelParams ) ) ) ; } pub type CUDA_LAUNCH_PARAMS = CUDA_LAUNCH_PARAMS_st ;